{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from vivarium_profiling.tools.extraction import ExtractionConfig\n",
    "from vivarium_profiling.tools import plotting\n",
    "\n",
    "# Configure matplotlib for notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7058668",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark results\n",
    "benchmark_results_path = Path(r\"{{BENCHMARK_RESULTS_PATH}}\")\n",
    "summary_path = Path(r\"{{SUMMARY_PATH}}\")\n",
    "\n",
    "raw = pd.read_csv(benchmark_results_path)\n",
    "summary = pd.read_csv(summary_path)\n",
    "\n",
    "# Load extraction config\n",
    "config = ExtractionConfig()\n",
    "\n",
    "print(f\"Loaded {len(raw)} raw benchmark results\")\n",
    "print(f\"Loaded {len(summary)} model summaries\")\n",
    "print(f\"\\nRaw data shape: {raw.shape}\")\n",
    "print(f\"Summary data shape: {summary.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c47df2b",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "Overall runtime and memory usage comparison across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.create_figures(\n",
    "    summary,\n",
    "    output_dir=None,\n",
    "    chart_title=\"performance_analysis\",\n",
    "    time_col=\"rt_s\",\n",
    "    mem_col=\"mem_mb\",\n",
    "    time_pdiff_col=\"rt_s_pdiff\",\n",
    "    save=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31e5f0",
   "metadata": {},
   "source": [
    "## Phase Runtime Analysis\n",
    "\n",
    "Detailed analysis of individual simulation phases (setup, initialize_simulants, run, finalize, report)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee250c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get phase metrics from config\n",
    "phase_patterns = [p for p in config.patterns if p.cumtime_template == \"rt_{name}_s\"]\n",
    "\n",
    "for pattern in phase_patterns:\n",
    "    time_col = pattern.cumtime_col\n",
    "    time_pdiff_col = f\"{time_col}_pdiff\"\n",
    "    \n",
    "    print(f\"\\n=== {pattern.name.upper()} ===\")\n",
    "    plotting.create_figures(\n",
    "        summary,\n",
    "        output_dir=None,\n",
    "        chart_title=f\"runtime_analysis_{pattern.name}\",\n",
    "        time_col=time_col,\n",
    "        mem_col=None,\n",
    "        time_pdiff_col=time_pdiff_col,\n",
    "        save=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07476e",
   "metadata": {},
   "source": [
    "## Non-Run Time Analysis\n",
    "\n",
    "Analysis of time spent outside the main run phase (setup, initialization, reporting, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.create_figures(\n",
    "    summary,\n",
    "    output_dir=None,\n",
    "    chart_title=\"runtime_analysis_non_run\",\n",
    "    time_col=\"rt_non_run_s\",\n",
    "    mem_col=None,\n",
    "    time_pdiff_col=\"rt_non_run_s_pdiff\",\n",
    "    save=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16a06d",
   "metadata": {},
   "source": [
    "## Bottleneck Cumulative Time Analysis\n",
    "\n",
    "Analysis of cumulative time spent in known bottleneck functions (gather_results, pipeline_call, population_get)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bottleneck patterns from config\n",
    "bottleneck_patterns = [\n",
    "    p for p in config.patterns\n",
    "    if p.extract_cumtime and p.cumtime_col == f\"{p.name}_cumtime\"\n",
    "]\n",
    "\n",
    "for pattern in bottleneck_patterns:\n",
    "    time_col = pattern.cumtime_col\n",
    "    time_pdiff_col = f\"{time_col}_pdiff\"\n",
    "    \n",
    "    print(f\"\\n=== {pattern.name.upper()} ===\")\n",
    "    plotting.create_figures(\n",
    "        summary,\n",
    "        output_dir=None,\n",
    "        chart_title=f\"bottleneck_runtime_analysis_{pattern.name}\",\n",
    "        time_col=time_col,\n",
    "        mem_col=None,\n",
    "        time_pdiff_col=time_pdiff_col,\n",
    "        save=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f267afb",
   "metadata": {},
   "source": [
    "## Bottleneck Fractions vs Scale Factor\n",
    "\n",
    "Fraction of run() time spent in each bottleneck function, plotted against model scale factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_bottleneck_fractions(\n",
    "    summary,\n",
    "    output_dir=None,\n",
    "    config=config,\n",
    "    metric=\"median\",\n",
    "    save=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
