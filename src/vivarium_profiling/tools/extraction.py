"""Data extraction utilities for benchmark profiling results.

This module provides configurable extraction of profiling metrics from cProfile
stats files and memory profiler output. The extraction is driven by BottleneckConfig
objects that map logical names to regex patterns for matching function calls.
"""

from __future__ import annotations

import re
import subprocess
from dataclasses import dataclass
from pathlib import Path
from typing import TYPE_CHECKING

import numpy as np
from loguru import logger

if TYPE_CHECKING:
    import pandas as pd


@dataclass
class BottleneckConfig:
    """Configuration for extracting metrics for a specific bottleneck function.

    Attributes
    ----------
    name
        Logical name for the bottleneck (e.g., "gather_results"). Used as prefix
        for generated column names.
    pattern
        Regex pattern to match the function in cProfile stats output.
        Should match the filename:lineno(function) format, e.g.,
        r"results/manager\\.py:\\d+\\(gather_results\\)"

    """

    name: str
    pattern: str

    @property
    def cumtime_col(self) -> str:
        """Column name for cumulative time."""
        return f"{self.name}_cumtime"

    @property
    def percall_col(self) -> str:
        """Column name for time per call."""
        return f"{self.name}_percall"

    @property
    def ncalls_col(self) -> str:
        """Column name for number of calls."""
        return f"{self.name}_ncalls"

    @property
    def columns(self) -> list[str]:
        """All column names generated by this bottleneck config."""
        return [self.cumtime_col, self.percall_col, self.ncalls_col]


# Default bottleneck configurations matching the original hardcoded patterns
DEFAULT_BOTTLENECKS: list[BottleneckConfig] = [
    BottleneckConfig(
        name="gather_results",
        pattern=r"results/manager\.py:\d+\(gather_results\)",
    ),
    BottleneckConfig(
        name="pipeline_call",
        pattern=r"values/pipeline\.py:\d+\(__call__\)",
    ),
    BottleneckConfig(
        name="population_get",
        pattern=r"population/population_view\.py:\d+\(get\)",
    ),
]


@dataclass
class SimulationPhaseConfig:
    """Configuration for extracting simulation phase timing.

    Attributes
    ----------
    name
        Phase name as it appears in the cProfile output (e.g., "setup", "run").
    file_pattern
        Pattern to match the source file containing the phase function.

    """

    name: str
    file_pattern: str = "/vivarium/framework/engine.py:"

    @property
    def column(self) -> str:
        """Column name for this phase's runtime."""
        return f"rt_{self.name}_s"


# Default simulation phases to extract
DEFAULT_SIMULATION_PHASES: list[SimulationPhaseConfig] = [
    SimulationPhaseConfig(name="setup"),
    SimulationPhaseConfig(name="initialize_simulants"),
    SimulationPhaseConfig(name="run"),
    SimulationPhaseConfig(name="finalize"),
    SimulationPhaseConfig(name="report"),
]


def get_bottleneck_columns(bottlenecks: list[BottleneckConfig] | None = None) -> list[str]:
    """Get all column names for the given bottleneck configurations.

    Parameters
    ----------
    bottlenecks
        List of bottleneck configurations. Defaults to DEFAULT_BOTTLENECKS.

    Returns
    -------
        List of column names for all bottleneck metrics.

    """
    if bottlenecks is None:
        bottlenecks = DEFAULT_BOTTLENECKS
    columns = []
    for config in bottlenecks:
        columns.extend(config.columns)
    return columns


def get_results_columns(bottlenecks: list[BottleneckConfig] | None = None) -> list[str]:
    """Get all column names for benchmark results CSV.

    Parameters
    ----------
    bottlenecks
        List of bottleneck configurations. Defaults to DEFAULT_BOTTLENECKS.

    Returns
    -------
        Complete list of column names for benchmark results.

    """
    base_columns = ["model_spec", "run", "rt_s", "mem_mb"]
    return base_columns + get_bottleneck_columns(bottlenecks)


def get_peak_memory() -> float | None:
    """Get peak memory usage from memory profiler.

    Calls `mprof peak` to retrieve the peak memory usage from the most recent
    memory profiler run.

    Returns
    -------
        Peak memory usage in MB, or None if extraction fails.

    """
    try:
        result = subprocess.run(["mprof", "peak"], capture_output=True, text=True, check=True)
        # Extract the first decimal number from the output
        match = re.search(r"(\d+\.\d+)", result.stdout)
        if match:
            return float(match.group(1))
    except (subprocess.CalledProcessError, ValueError):
        logger.warning("Could not extract peak memory usage")
    return None


def extract_runtime(stats_file_txt: str | Path) -> float | None:
    """Extract total runtime from a cProfile stats file.

    Parameters
    ----------
    stats_file_txt
        Path to the .stats.txt file generated by cProfile.

    Returns
    -------
        Total runtime in seconds, or None if extraction fails.

    """
    try:
        with open(stats_file_txt, "r") as f:
            content = f.read()

        # Look for pattern like "12345 function calls (12340 primitive calls) in 1.234 seconds"
        match = re.search(r"function calls.*in (\d+\.\d+) seconds", content)
        if match:
            return float(match.group(1))
    except (FileNotFoundError, ValueError, OSError) as e:
        logger.warning(f"Could not extract runtime from {stats_file_txt}: {e}")
    return None


def parse_function_metrics(
    stats_file_txt: str | Path, pattern: str
) -> tuple[float | None, float | None, int | None]:
    """Parse cumtime, percall, and ncalls for a specific function pattern.

    Parameters
    ----------
    stats_file_txt
        Path to the .stats.txt file generated by cProfile.
    pattern
        Regex pattern to match the function in the stats output.

    Returns
    -------
        Tuple of (cumtime, percall, ncalls), with None values if extraction fails.

    """
    try:
        with open(stats_file_txt, "r") as f:
            content = f.read()

        # Find the line matching the pattern
        lines = content.split("\n")
        matching_line = None
        for line in lines:
            if re.search(pattern, line):
                matching_line = line
                break

        if not matching_line:
            return None, None, None

        # Parse the line - typical format:
        # "ncalls  tottime  percall  cumtime  percall filename:lineno(function)"
        parts = matching_line.split()
        if len(parts) >= 5:
            ncalls_str = parts[0]
            cumtime = float(parts[3])
            percall = float(parts[4])

            # Handle ncalls which might be in format "123/456" (recursive calls)
            if "/" in ncalls_str:
                ncalls = int(ncalls_str.split("/")[1])
            else:
                ncalls = int(ncalls_str)

            return cumtime, percall, ncalls

    except (FileNotFoundError, ValueError, IndexError, OSError) as e:
        logger.warning(
            f"Could not parse function metrics from {stats_file_txt} with pattern {pattern}: {e}"
        )

    return None, None, None


def extract_bottleneck_metrics(
    stats_file_txt: str | Path,
    bottlenecks: list[BottleneckConfig] | None = None,
) -> dict[str, float | int | None]:
    """Extract metrics for all configured bottlenecks from a stats file.

    Parameters
    ----------
    stats_file_txt
        Path to the .stats.txt file generated by cProfile.
    bottlenecks
        List of bottleneck configurations. Defaults to DEFAULT_BOTTLENECKS.

    Returns
    -------
        Dictionary mapping column names to extracted values.

    """
    if bottlenecks is None:
        bottlenecks = DEFAULT_BOTTLENECKS

    results: dict[str, float | int | None] = {}
    for config in bottlenecks:
        cumtime, percall, ncalls = parse_function_metrics(stats_file_txt, config.pattern)
        results[config.cumtime_col] = cumtime
        results[config.percall_col] = percall
        results[config.ncalls_col] = ncalls

    return results


def extract_simulation_phase_times(
    raw: "pd.DataFrame",
    output_dir: Path,
    phases: list[SimulationPhaseConfig] | None = None,
) -> "pd.DataFrame":
    """Extract simulation phase runtimes from cProfile stats files.

    This function re-parses the .stats.txt files to extract per-phase cumulative
    times that were not captured during the initial benchmark run.

    Parameters
    ----------
    raw
        DataFrame with benchmark results containing 'model_spec' and 'run' columns.
    output_dir
        Directory containing the profiling results (with subdirectories per model_spec).
    phases
        List of simulation phase configurations. Defaults to DEFAULT_SIMULATION_PHASES.

    Returns
    -------
        DataFrame with additional columns for each phase's runtime.

    Raises
    ------
    FileNotFoundError
        If the expected results directory for a model spec does not exist.
    RuntimeError
        If there are data integrity issues (missing or duplicate runs).

    """
    import numpy as np

    if phases is None:
        phases = DEFAULT_SIMULATION_PHASES

    phase_funcs = [phase.name for phase in phases]
    col_map = {phase.name: phase.column for phase in phases}

    # Initialize columns with NaN
    for col in col_map.values():
        raw[col] = np.nan

    for model_spec in raw["model_spec"].unique():
        spec_stem = Path(model_spec).stem
        spec_results_dir = output_dir / spec_stem

        if not spec_results_dir.exists():
            raise FileNotFoundError(
                f"Expected results directory '{spec_results_dir}' for model spec "
                f"'{model_spec}' not found. Please ensure the profiling script has "
                "run successfully and generated the expected output directories."
            )

        sorted_stats_files = sorted(
            file
            for file in spec_results_dir.rglob(f"{spec_stem}.stats.txt")
            if file.is_file()
        )

        for run_number, stats_file in enumerate(sorted_stats_files, start=1):
            with stats_file.open("r") as fh:
                for line in fh:
                    # Check if line is from the expected source file
                    if not any(phase.file_pattern in line for phase in phases):
                        continue

                    # Check if line contains a phase function call
                    if not any(f"({phase})" in line for phase in phase_funcs):
                        continue

                    phase_name = line.split()[-1].split("(")[-1].split(")")[0]
                    phase_rt = float(line.split()[3])  # cumtime is the 4th item

                    mask = (raw["model_spec"] == model_spec) & (raw["run"] == run_number)

                    if mask.sum() == 0:
                        raise RuntimeError(
                            f"No matching run found for model_spec '{model_spec}' "
                            f"and run_number {run_number}. Please check the profiling "
                            "script output and ensure runs are correctly recorded."
                        )
                    if mask.sum() > 1:
                        raise RuntimeError(
                            f"Multiple runs found for model_spec '{model_spec}' "
                            f"and run_number {run_number}. This should not happen; "
                            "please check the profiling script output."
                        )

                    raw.loc[mask, col_map[phase_name]] = phase_rt

    return raw


def get_bottleneck_names(bottlenecks: list[BottleneckConfig] | None = None) -> list[str]:
    """Get the names of all configured bottlenecks.

    Parameters
    ----------
    bottlenecks
        List of bottleneck configurations. Defaults to DEFAULT_BOTTLENECKS.

    Returns
    -------
        List of bottleneck names.

    """
    if bottlenecks is None:
        bottlenecks = DEFAULT_BOTTLENECKS
    return [config.name for config in bottlenecks]
